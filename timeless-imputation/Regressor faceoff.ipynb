{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import utils\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pickle_utils as pu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.439541</td>\n",
       "      <td>0.232202</td>\n",
       "      <td>-1.318699</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>-0.124403</td>\n",
       "      <td>0.498903</td>\n",
       "      <td>-0.072483</td>\n",
       "      <td>0.128169</td>\n",
       "      <td>-0.979145</td>\n",
       "      <td>-0.672666</td>\n",
       "      <td>-1.536773</td>\n",
       "      <td>0.440875</td>\n",
       "      <td>-1.087087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.610376</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254145</td>\n",
       "      <td>0.409962</td>\n",
       "      <td>0.537514</td>\n",
       "      <td>-0.863680</td>\n",
       "      <td>-0.996071</td>\n",
       "      <td>-0.352908</td>\n",
       "      <td>0.440875</td>\n",
       "      <td>-0.507591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.437256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.610376</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.216865</td>\n",
       "      <td>0.537514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.352908</td>\n",
       "      <td>0.398234</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.436702</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.755654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417093</td>\n",
       "      <td>-1.371263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>-0.786392</td>\n",
       "      <td>1.408002</td>\n",
       "      <td>-0.459848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.748215</td>\n",
       "      <td>-1.115851</td>\n",
       "      <td>0.073283</td>\n",
       "      <td>0.440875</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.436977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.338044</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.048600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.411749</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.430607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.395892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.814455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.582831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.427255</td>\n",
       "      <td>-0.049288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.424476</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>-0.491330</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>-0.240251</td>\n",
       "      <td>-0.141599</td>\n",
       "      <td>1.015660</td>\n",
       "      <td>0.996469</td>\n",
       "      <td>-0.517284</td>\n",
       "      <td>-0.582831</td>\n",
       "      <td>-1.584128</td>\n",
       "      <td>0.440875</td>\n",
       "      <td>0.886821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>-0.240251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.517284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333277</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>-0.240251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.584128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>-0.240251</td>\n",
       "      <td>0.184215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.181377</td>\n",
       "      <td>-0.517284</td>\n",
       "      <td>-0.582831</td>\n",
       "      <td>-1.584128</td>\n",
       "      <td>0.394986</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.427427</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>-0.491330</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>-0.240251</td>\n",
       "      <td>-0.400660</td>\n",
       "      <td>0.550822</td>\n",
       "      <td>1.125373</td>\n",
       "      <td>-0.517284</td>\n",
       "      <td>-0.582831</td>\n",
       "      <td>-1.584128</td>\n",
       "      <td>0.440875</td>\n",
       "      <td>0.067726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.430009</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>-0.491330</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>-0.240251</td>\n",
       "      <td>-0.591380</td>\n",
       "      <td>-0.995116</td>\n",
       "      <td>0.763305</td>\n",
       "      <td>-0.517284</td>\n",
       "      <td>-0.582831</td>\n",
       "      <td>-1.584128</td>\n",
       "      <td>0.373823</td>\n",
       "      <td>0.407622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.451152</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.192214</td>\n",
       "      <td>0.416358</td>\n",
       "      <td>-0.632749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.162439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.370699</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>-0.451152</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>-0.124403</td>\n",
       "      <td>-0.262388</td>\n",
       "      <td>0.607166</td>\n",
       "      <td>0.301736</td>\n",
       "      <td>-0.632749</td>\n",
       "      <td>-0.606787</td>\n",
       "      <td>1.162439</td>\n",
       "      <td>0.264024</td>\n",
       "      <td>-0.351573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.371851</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>-0.451152</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>-0.124403</td>\n",
       "      <td>-0.678794</td>\n",
       "      <td>-0.378854</td>\n",
       "      <td>0.318864</td>\n",
       "      <td>-0.632749</td>\n",
       "      <td>-0.606787</td>\n",
       "      <td>1.162439</td>\n",
       "      <td>0.427465</td>\n",
       "      <td>-0.600924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.325363</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>-0.451152</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>-0.124403</td>\n",
       "      <td>-0.518271</td>\n",
       "      <td>-1.336701</td>\n",
       "      <td>0.318864</td>\n",
       "      <td>-0.632749</td>\n",
       "      <td>-0.606787</td>\n",
       "      <td>1.162439</td>\n",
       "      <td>0.335582</td>\n",
       "      <td>-0.864204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.354761</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>-0.451152</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>-0.124403</td>\n",
       "      <td>-0.430858</td>\n",
       "      <td>0.508564</td>\n",
       "      <td>0.206528</td>\n",
       "      <td>-0.632749</td>\n",
       "      <td>-0.606787</td>\n",
       "      <td>1.162439</td>\n",
       "      <td>0.334534</td>\n",
       "      <td>0.262748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.079632</td>\n",
       "      <td>-0.008808</td>\n",
       "      <td>-0.632749</td>\n",
       "      <td>-0.606787</td>\n",
       "      <td>1.162439</td>\n",
       "      <td>-0.689691</td>\n",
       "      <td>-0.152371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.361126</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>-0.451152</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>-0.124403</td>\n",
       "      <td>-0.848852</td>\n",
       "      <td>0.078941</td>\n",
       "      <td>-0.008808</td>\n",
       "      <td>-0.632749</td>\n",
       "      <td>-0.606787</td>\n",
       "      <td>1.162439</td>\n",
       "      <td>0.378537</td>\n",
       "      <td>-0.209485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.303798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.451152</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.086090</td>\n",
       "      <td>-0.008155</td>\n",
       "      <td>-0.632749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.162439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.147316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.347367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.451152</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.305904</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.189279</td>\n",
       "      <td>0.860714</td>\n",
       "      <td>0.075385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.826921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.332502</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>-0.451152</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>-0.124403</td>\n",
       "      <td>-0.712170</td>\n",
       "      <td>1.152998</td>\n",
       "      <td>0.130596</td>\n",
       "      <td>-0.632749</td>\n",
       "      <td>-0.606787</td>\n",
       "      <td>1.162439</td>\n",
       "      <td>0.416150</td>\n",
       "      <td>0.988512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.358460</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>-0.451152</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>-0.124403</td>\n",
       "      <td>-0.535754</td>\n",
       "      <td>0.945230</td>\n",
       "      <td>0.272660</td>\n",
       "      <td>-0.632749</td>\n",
       "      <td>-0.606787</td>\n",
       "      <td>1.162439</td>\n",
       "      <td>0.413950</td>\n",
       "      <td>0.489811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.348620</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>-0.451152</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>-0.124403</td>\n",
       "      <td>-1.052287</td>\n",
       "      <td>0.649424</td>\n",
       "      <td>0.298329</td>\n",
       "      <td>-0.632749</td>\n",
       "      <td>-0.606787</td>\n",
       "      <td>1.162439</td>\n",
       "      <td>-0.538509</td>\n",
       "      <td>0.519064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.366999</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.712170</td>\n",
       "      <td>0.811413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.632749</td>\n",
       "      <td>-0.606787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.336061</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297769</td>\n",
       "      <td>-0.632749</td>\n",
       "      <td>-0.606787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.626327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.355982</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>-0.451152</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>-0.124403</td>\n",
       "      <td>0.371756</td>\n",
       "      <td>0.955794</td>\n",
       "      <td>0.298375</td>\n",
       "      <td>-0.632749</td>\n",
       "      <td>-0.606787</td>\n",
       "      <td>1.162439</td>\n",
       "      <td>0.347002</td>\n",
       "      <td>0.002254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.330974</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>-0.451152</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>-0.124403</td>\n",
       "      <td>0.656247</td>\n",
       "      <td>0.705768</td>\n",
       "      <td>0.197708</td>\n",
       "      <td>-0.632749</td>\n",
       "      <td>-0.606787</td>\n",
       "      <td>1.162439</td>\n",
       "      <td>0.266225</td>\n",
       "      <td>-0.111974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.090702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.030969</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>0.504486</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.543257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433646</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>1.197162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.030969</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>0.504486</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.057918</td>\n",
       "      <td>-0.800243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.055941</td>\n",
       "      <td>1.689200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.120937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.767667</td>\n",
       "      <td>1.676555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.783602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>1.030969</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>0.504486</td>\n",
       "      <td>-0.051007</td>\n",
       "      <td>0.730418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.676555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.783602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>0.194525</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>-0.174053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.182562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.543257</td>\n",
       "      <td>0.783602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.284708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>0.181901</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>1.030969</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>-0.174053</td>\n",
       "      <td>0.777036</td>\n",
       "      <td>0.269102</td>\n",
       "      <td>-0.225732</td>\n",
       "      <td>1.676555</td>\n",
       "      <td>1.543257</td>\n",
       "      <td>0.783602</td>\n",
       "      <td>0.400749</td>\n",
       "      <td>-0.702614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>0.184406</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>1.030969</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>-0.174053</td>\n",
       "      <td>1.271319</td>\n",
       "      <td>0.343054</td>\n",
       "      <td>-0.188909</td>\n",
       "      <td>1.676555</td>\n",
       "      <td>1.543257</td>\n",
       "      <td>0.783602</td>\n",
       "      <td>0.423903</td>\n",
       "      <td>-0.804304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>-0.133056</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>1.030969</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.949336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.676555</td>\n",
       "      <td>1.543257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.399177</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>-0.180991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.619988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.676555</td>\n",
       "      <td>1.543257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166694</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>-0.039839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.030969</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>0.247965</td>\n",
       "      <td>0.080908</td>\n",
       "      <td>-0.540842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.543257</td>\n",
       "      <td>0.783602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.030969</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.441656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.676555</td>\n",
       "      <td>1.543257</td>\n",
       "      <td>0.783602</td>\n",
       "      <td>0.396663</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.565951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.309458</td>\n",
       "      <td>1.676555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>-0.423788</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>2.465472</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>0.463111</td>\n",
       "      <td>-1.282741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.930826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.812761</td>\n",
       "      <td>0.736247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>-0.420245</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>2.465472</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>0.463111</td>\n",
       "      <td>-1.346314</td>\n",
       "      <td>1.093133</td>\n",
       "      <td>-0.961395</td>\n",
       "      <td>-0.632749</td>\n",
       "      <td>1.812761</td>\n",
       "      <td>0.736247</td>\n",
       "      <td>-0.112831</td>\n",
       "      <td>1.558257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>2.465472</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.856490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.736247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>-0.428706</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>2.465472</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>0.463111</td>\n",
       "      <td>-0.441983</td>\n",
       "      <td>1.110740</td>\n",
       "      <td>-0.908798</td>\n",
       "      <td>-0.632749</td>\n",
       "      <td>1.812761</td>\n",
       "      <td>0.736247</td>\n",
       "      <td>0.369737</td>\n",
       "      <td>0.736375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>-0.428097</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>2.465472</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>0.463111</td>\n",
       "      <td>-0.441983</td>\n",
       "      <td>0.571951</td>\n",
       "      <td>-0.795949</td>\n",
       "      <td>-0.632749</td>\n",
       "      <td>1.812761</td>\n",
       "      <td>0.736247</td>\n",
       "      <td>0.440875</td>\n",
       "      <td>0.078870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.669099</td>\n",
       "      <td>-0.401819</td>\n",
       "      <td>-0.103713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>-0.409760</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>-0.220501</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>0.264515</td>\n",
       "      <td>-0.532575</td>\n",
       "      <td>-0.868342</td>\n",
       "      <td>-0.669099</td>\n",
       "      <td>-0.401819</td>\n",
       "      <td>-0.103713</td>\n",
       "      <td>0.310056</td>\n",
       "      <td>0.440875</td>\n",
       "      <td>0.112302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>-0.420722</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>-0.220501</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>0.264515</td>\n",
       "      <td>-0.939444</td>\n",
       "      <td>-1.354309</td>\n",
       "      <td>-0.474531</td>\n",
       "      <td>-0.401819</td>\n",
       "      <td>-0.103713</td>\n",
       "      <td>0.310056</td>\n",
       "      <td>0.403053</td>\n",
       "      <td>0.670903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-0.408667</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>-0.220501</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>0.264515</td>\n",
       "      <td>-1.384458</td>\n",
       "      <td>0.198672</td>\n",
       "      <td>-0.474531</td>\n",
       "      <td>-0.401819</td>\n",
       "      <td>-0.103713</td>\n",
       "      <td>0.310056</td>\n",
       "      <td>0.440875</td>\n",
       "      <td>1.164032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>-0.410980</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>-0.220501</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>0.264515</td>\n",
       "      <td>-0.742367</td>\n",
       "      <td>0.117678</td>\n",
       "      <td>-0.430614</td>\n",
       "      <td>-0.401819</td>\n",
       "      <td>-0.103713</td>\n",
       "      <td>0.310056</td>\n",
       "      <td>0.440875</td>\n",
       "      <td>0.183346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>-0.414169</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>-0.220501</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>0.264515</td>\n",
       "      <td>-0.384767</td>\n",
       "      <td>-0.068962</td>\n",
       "      <td>-0.656312</td>\n",
       "      <td>-0.401819</td>\n",
       "      <td>-0.103713</td>\n",
       "      <td>0.310056</td>\n",
       "      <td>0.440875</td>\n",
       "      <td>0.018970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219801</td>\n",
       "      <td>-0.660605</td>\n",
       "      <td>-0.401819</td>\n",
       "      <td>-0.103713</td>\n",
       "      <td>0.310056</td>\n",
       "      <td>0.429036</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>-0.415775</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>-0.220501</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>0.264515</td>\n",
       "      <td>-0.372052</td>\n",
       "      <td>0.438134</td>\n",
       "      <td>-0.614728</td>\n",
       "      <td>-0.401819</td>\n",
       "      <td>-0.103713</td>\n",
       "      <td>0.310056</td>\n",
       "      <td>0.440875</td>\n",
       "      <td>0.215386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.162439</td>\n",
       "      <td>0.389433</td>\n",
       "      <td>-0.433761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>-0.435296</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>0.112828</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.713063</td>\n",
       "      <td>-0.979145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.440875</td>\n",
       "      <td>-0.515949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.836063</td>\n",
       "      <td>-0.769067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.162439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.995148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>-0.428286</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.477902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.776198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.979145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.162439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>-0.435063</td>\n",
       "      <td>-0.519476</td>\n",
       "      <td>0.112828</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>0.165217</td>\n",
       "      <td>-0.367284</td>\n",
       "      <td>0.476871</td>\n",
       "      <td>-0.611555</td>\n",
       "      <td>-0.979145</td>\n",
       "      <td>-0.810413</td>\n",
       "      <td>1.162439</td>\n",
       "      <td>0.440875</td>\n",
       "      <td>-0.683112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         crim        zn     indus      chas       nox        rm       age  \\\n",
       "1   -0.439541  0.232202 -1.318699  0.675028 -0.124403  0.498903 -0.072483   \n",
       "2    0.000000  0.000000 -0.610376  0.675028  0.000000  0.254145  0.409962   \n",
       "3   -0.437256  0.000000 -0.610376 -1.477902  0.000000  0.000000 -0.216865   \n",
       "4   -0.436702 -0.519476  0.000000 -1.477902  0.000000  0.000000 -0.755654   \n",
       "5    0.000000  0.000000  0.000000  0.675028 -0.786392  1.408002 -0.459848   \n",
       "6   -0.436977  0.000000 -1.338044  0.675028  0.000000  0.000000  0.000000   \n",
       "7   -0.430607  0.000000  0.000000 -1.477902  0.000000 -0.395892  0.000000   \n",
       "8   -0.424476  0.002522 -0.491330  0.675028 -0.240251 -0.141599  1.015660   \n",
       "9    0.000000  0.000000  0.000000 -1.477902 -0.240251  0.000000  0.000000   \n",
       "10   0.000000  0.002522  0.000000 -1.477902 -0.240251  0.000000  0.000000   \n",
       "11   0.000000  0.000000  0.000000  0.675028 -0.240251  0.184215  0.000000   \n",
       "12  -0.427427  0.002522 -0.491330  0.675028 -0.240251 -0.400660  0.550822   \n",
       "13  -0.430009  0.002522 -0.491330  0.675028 -0.240251 -0.591380 -0.995116   \n",
       "14   0.000000  0.000000 -0.451152 -1.477902  0.000000  0.000000 -0.192214   \n",
       "15  -0.370699 -0.519476 -0.451152  0.675028 -0.124403 -0.262388  0.607166   \n",
       "16  -0.371851 -0.519476 -0.451152  0.675028 -0.124403 -0.678794 -0.378854   \n",
       "17  -0.325363 -0.519476 -0.451152  0.675028 -0.124403 -0.518271 -1.336701   \n",
       "18  -0.354761 -0.519476 -0.451152  0.675028 -0.124403 -0.430858  0.508564   \n",
       "19   0.000000  0.000000  0.000000 -1.477902  0.000000  0.000000 -1.079632   \n",
       "20  -0.361126 -0.519476 -0.451152  0.675028 -0.124403 -0.848852  0.078941   \n",
       "21  -0.303798  0.000000 -0.451152 -1.477902  0.000000  0.000000  1.086090   \n",
       "22  -0.347367  0.000000 -0.451152 -1.477902  0.000000  0.000000  0.000000   \n",
       "23  -0.305904 -0.519476  0.000000 -1.477902  0.000000 -0.189279  0.860714   \n",
       "24  -0.332502 -0.519476 -0.451152  0.675028 -0.124403 -0.712170  1.152998   \n",
       "25  -0.358460 -0.519476 -0.451152  0.675028 -0.124403 -0.535754  0.945230   \n",
       "26  -0.348620 -0.519476 -0.451152  0.675028 -0.124403 -1.052287  0.649424   \n",
       "27  -0.366999 -0.519476  0.000000  0.675028  0.000000 -0.712170  0.811413   \n",
       "28  -0.336061 -0.519476  0.000000 -1.477902  0.000000  0.000000  0.000000   \n",
       "29  -0.355982 -0.519476 -0.451152  0.675028 -0.124403  0.371756  0.955794   \n",
       "30  -0.330974 -0.519476 -0.451152  0.675028 -0.124403  0.656247  0.705768   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "477  0.090702  0.000000  1.030969 -1.477902  0.504486  0.000000  0.000000   \n",
       "478  1.197162  0.000000  1.030969  0.675028  0.504486  0.000000  1.057918   \n",
       "479  0.000000  0.000000  0.000000 -1.477902  0.000000 -0.120937  0.000000   \n",
       "480  0.000000 -0.519476  1.030969  0.675028  0.504486 -0.051007  0.730418   \n",
       "481  0.194525 -0.519476  0.000000 -1.477902 -0.174053  0.000000  0.000000   \n",
       "482  0.181901 -0.519476  1.030969  0.675028 -0.174053  0.777036  0.269102   \n",
       "483  0.184406 -0.519476  1.030969  0.675028 -0.174053  1.271319  0.343054   \n",
       "484 -0.133056 -0.519476  1.030969 -1.477902  0.000000  0.000000 -0.949336   \n",
       "485 -0.180991  0.000000  0.000000  0.675028  0.000000 -0.619988  0.000000   \n",
       "486 -0.039839  0.000000  1.030969  0.675028  0.247965  0.080908 -0.540842   \n",
       "487  0.000000  0.000000  1.030969 -1.477902  0.000000  0.000000  0.441656   \n",
       "488  0.000000  0.000000  0.000000 -1.477902  0.000000 -0.565951  0.000000   \n",
       "489 -0.423788 -0.519476  2.465472 -1.477902  0.463111 -1.282741  0.000000   \n",
       "490 -0.420245 -0.519476  2.465472  0.675028  0.463111 -1.346314  1.093133   \n",
       "491  0.000000 -0.519476  2.465472  0.675028  0.000000 -1.856490  0.000000   \n",
       "492 -0.428706 -0.519476  2.465472  0.675028  0.463111 -0.441983  1.110740   \n",
       "493 -0.428097 -0.519476  2.465472  0.675028  0.463111 -0.441983  0.571951   \n",
       "494  0.000000 -0.519476  0.000000 -1.477902  0.000000  0.000000  0.000000   \n",
       "495 -0.409760 -0.519476 -0.220501  0.675028  0.264515 -0.532575 -0.868342   \n",
       "496 -0.420722 -0.519476 -0.220501  0.675028  0.264515 -0.939444 -1.354309   \n",
       "497 -0.408667 -0.519476 -0.220501  0.675028  0.264515 -1.384458  0.198672   \n",
       "498 -0.410980 -0.519476 -0.220501  0.675028  0.264515 -0.742367  0.117678   \n",
       "499 -0.414169 -0.519476 -0.220501  0.675028  0.264515 -0.384767 -0.068962   \n",
       "500  0.000000 -0.519476  0.000000 -1.477902  0.000000  0.000000  0.219801   \n",
       "501 -0.415775 -0.519476 -0.220501  0.675028  0.264515 -0.372052  0.438134   \n",
       "502  0.000000  0.000000  0.000000  0.675028  0.000000  0.000000  0.064855   \n",
       "503 -0.435296 -0.519476  0.112828 -1.477902  0.000000  0.000000  0.000000   \n",
       "504  0.000000  0.000000  0.000000  0.675028  0.000000  0.000000  0.836063   \n",
       "505 -0.428286 -0.519476  0.000000 -1.477902  0.000000  0.000000  0.776198   \n",
       "506 -0.435063 -0.519476  0.112828  0.675028  0.165217 -0.367284  0.476871   \n",
       "\n",
       "          dis       rad       tax   ptratio         b     lstat  \n",
       "1    0.128169 -0.979145 -0.672666 -1.536773  0.440875 -1.087087  \n",
       "2    0.537514 -0.863680 -0.996071 -0.352908  0.440875 -0.507591  \n",
       "3    0.537514  0.000000  0.000000 -0.352908  0.398234  0.000000  \n",
       "4    0.000000  0.000000  0.000000  0.000000  0.417093 -1.371263  \n",
       "5    0.000000 -0.748215 -1.115851  0.073283  0.440875  0.000000  \n",
       "6    1.048600  0.000000  0.000000  0.000000  0.411749  0.000000  \n",
       "7    0.814455  0.000000 -0.582831  0.000000  0.427255 -0.049288  \n",
       "8    0.996469 -0.517284 -0.582831 -1.584128  0.440875  0.886821  \n",
       "9    0.000000 -0.517284  0.000000  0.000000  0.333277  0.000000  \n",
       "10   0.000000  0.000000  0.000000 -1.584128  0.000000  0.000000  \n",
       "11   1.181377 -0.517284 -0.582831 -1.584128  0.394986  0.000000  \n",
       "12   1.125373 -0.517284 -0.582831 -1.584128  0.440875  0.067726  \n",
       "13   0.763305 -0.517284 -0.582831 -1.584128  0.373823  0.407622  \n",
       "14   0.416358 -0.632749  0.000000  1.162439  0.000000  0.000000  \n",
       "15   0.301736 -0.632749 -0.606787  1.162439  0.264024 -0.351573  \n",
       "16   0.318864 -0.632749 -0.606787  1.162439  0.427465 -0.600924  \n",
       "17   0.318864 -0.632749 -0.606787  1.162439  0.335582 -0.864204  \n",
       "18   0.206528 -0.632749 -0.606787  1.162439  0.334534  0.262748  \n",
       "19  -0.008808 -0.632749 -0.606787  1.162439 -0.689691 -0.152371  \n",
       "20  -0.008808 -0.632749 -0.606787  1.162439  0.378537 -0.209485  \n",
       "21  -0.008155 -0.632749  0.000000  1.162439  0.000000  1.147316  \n",
       "22   0.000000  0.000000  0.000000  0.000000  0.000000  0.145735  \n",
       "23   0.075385  0.000000  0.000000  0.000000  0.000000  0.826921  \n",
       "24   0.130596 -0.632749 -0.606787  1.162439  0.416150  0.988512  \n",
       "25   0.272660 -0.632749 -0.606787  1.162439  0.413950  0.489811  \n",
       "26   0.298329 -0.632749 -0.606787  1.162439 -0.538509  0.519064  \n",
       "27   0.000000 -0.632749 -0.606787  0.000000  0.000000  0.000000  \n",
       "28   0.297769 -0.632749 -0.606787  0.000000  0.000000  0.626327  \n",
       "29   0.298375 -0.632749 -0.606787  1.162439  0.347002  0.002254  \n",
       "30   0.197708 -0.632749 -0.606787  1.162439  0.266225 -0.111974  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "477  0.000000  0.000000  1.543257  0.000000  0.433646  0.000000  \n",
       "478 -0.800243  0.000000  0.000000  0.000000 -0.055941  1.689200  \n",
       "479 -0.767667  1.676555  0.000000  0.783602  0.000000  0.000000  \n",
       "480  0.000000  1.676555  0.000000  0.783602  0.000000  0.000000  \n",
       "481 -0.182562  0.000000  1.543257  0.783602  0.000000 -0.284708  \n",
       "482 -0.225732  1.676555  1.543257  0.783602  0.400749 -0.702614  \n",
       "483 -0.188909  1.676555  1.543257  0.783602  0.423903 -0.804304  \n",
       "484  0.000000  1.676555  1.543257  0.000000  0.399177  0.000000  \n",
       "485  0.000000  1.676555  1.543257  0.000000  0.166694  0.000000  \n",
       "486  0.000000  0.000000  1.543257  0.783602  0.000000  0.000000  \n",
       "487  0.000000  1.676555  1.543257  0.783602  0.396663  0.000000  \n",
       "488 -0.309458  1.676555  0.000000  0.000000  0.000000  0.000000  \n",
       "489 -0.930826  0.000000  1.812761  0.736247  0.000000  0.000000  \n",
       "490 -0.961395 -0.632749  1.812761  0.736247 -0.112831  1.558257  \n",
       "491  0.000000  0.000000  0.000000  0.736247  0.000000  0.000000  \n",
       "492 -0.908798 -0.632749  1.812761  0.736247  0.369737  0.736375  \n",
       "493 -0.795949 -0.632749  1.812761  0.736247  0.440875  0.078870  \n",
       "494 -0.669099 -0.401819 -0.103713  0.000000  0.000000  0.000000  \n",
       "495 -0.669099 -0.401819 -0.103713  0.310056  0.440875  0.112302  \n",
       "496 -0.474531 -0.401819 -0.103713  0.310056  0.403053  0.670903  \n",
       "497 -0.474531 -0.401819 -0.103713  0.310056  0.440875  1.164032  \n",
       "498 -0.430614 -0.401819 -0.103713  0.310056  0.440875  0.183346  \n",
       "499 -0.656312 -0.401819 -0.103713  0.310056  0.440875  0.018970  \n",
       "500 -0.660605 -0.401819 -0.103713  0.310056  0.429036  0.000000  \n",
       "501 -0.614728 -0.401819 -0.103713  0.310056  0.440875  0.215386  \n",
       "502  0.000000  0.000000  0.000000  1.162439  0.389433 -0.433761  \n",
       "503 -0.713063 -0.979145  0.000000  0.000000  0.440875 -0.515949  \n",
       "504 -0.769067  0.000000  0.000000  1.162439  0.000000 -0.995148  \n",
       "505  0.000000 -0.979145  0.000000  1.162439  0.000000  0.000000  \n",
       "506 -0.611555 -0.979145 -0.810413  1.162439  0.440875 -0.683112  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dset = datasets.datasets()[\"BostonHousing\"][0].astype(np.float64)\n",
    "dset = pu.load(\"impute_benchmark/amputed_BostonHousing_MCAR_rows_0.3.pkl.gz\")\n",
    "test_mask = np.random.rand(len(dset)) < 0.2\n",
    "\n",
    "norm_mean = dset[~test_mask].mean()\n",
    "norm_std = dset[~test_mask].std()\n",
    "\n",
    "dset = (dset - norm_mean) / norm_std\n",
    "dset = dset.fillna(0)\n",
    "\n",
    "y_train = dset.values[~test_mask, 0]\n",
    "X_train = dset.values[~test_mask, 1:]\n",
    "y_test = dset.values[test_mask, 0]\n",
    "X_test = dset.values[test_mask, 1:]\n",
    "\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(np.isfinite(y_train)), np.all(np.isfinite(X_train)), np.all(np.isfinite(y_test)), np.all(np.isfinite(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 0.2045, Training: 0.9087\n",
      "Train: 0.671255061871\n",
      "Test: 0.980223486398\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, n_jobs=-1,\n",
    "                           max_features=int(np.floor(X_train.shape[1]**.5)),\n",
    "                           bootstrap=False,\n",
    "                           min_samples_split=5)\n",
    "rf.fit(X_train, y_train)\n",
    "train_perf = rf.score(X_train, y_train)\n",
    "test_perf = rf.score(X_test, y_test)\n",
    "print(\"Test: {:.4f}, Training: {:.4f}\".format(test_perf, train_perf))\n",
    "y, _ = m.predict(X_train)\n",
    "print(\"Train:\", rmse(y_train, y.flatten()))\n",
    "y, _ = m.predict(X_test)\n",
    "print(\"Test:\", rmse(y_test, y.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def r_squared(y_true, y):\n",
    "    u = np.sum((y-y_true)**2)\n",
    "    v = np.sum((y_true - np.mean(y_true))**2)\n",
    "    return 1 - u/v\n",
    "def rmse(y_true, y):\n",
    "    return np.mean((y_true - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.488219156171\n",
      "Test: 0.930104888792\n"
     ]
    }
   ],
   "source": [
    "import GPy\n",
    "m = GPy.models.SparseGPRegression(X_train, y_train[:,np.newaxis],\n",
    "                            kernel=GPy.kern.Matern52(X_train.shape[1], ARD=False) + GPy.kern.White(X_train.shape[1]))\n",
    "m.optimize()\n",
    "y, _ = m.predict(X_train)\n",
    "print(\"Train:\", rmse(y_train, y.flatten()))\n",
    "y, _ = m.predict(X_test)\n",
    "print(\"Test:\", rmse(y_test, y.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax',\n",
       "       'ptratio', 'b', 'lstat'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import selu\n",
    "import denoising_ae as dae\n",
    "import imp\n",
    "imp.reload(dae)\n",
    "imp.reload(selu)\n",
    "\n",
    "tf_float = tf.float32\n",
    "tf.reset_default_graph()\n",
    "\n",
    "nn_X = tf.placeholder(tf_float, shape=[None, X_train.shape[1]], name=\"X\")\n",
    "nn_y = tf.placeholder(tf_float, shape=[None, 1], name=\"y\")\n",
    "nn_kp = tf.placeholder(tf_float, shape=[], name=\"keep_prob\")\n",
    "nn_lr = tf.placeholder(tf_float, shape=[], name=\"learning_rate\")\n",
    "\n",
    "\n",
    "nn_preds = dae.FC_net(nn_X, [(1024, selu.nlin)]*8 + [(1, None)], selu.initializer, keep_prob=nn_kp)\n",
    "nn_loss = tf.reduce_mean(tf.squared_difference(nn_preds, nn_y))\n",
    "nn_train = tf.train.GradientDescentOptimizer(nn_lr).minimize(nn_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 1it [00:00,  3.25it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 3it [00:00,  4.23it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 5it [00:00,  5.52it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 8it [00:00,  7.19it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 11it [00:00,  9.28it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 14it [00:00, 11.47it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 17it [00:01, 13.87it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 20it [00:01, 16.15it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 23it [00:01, 18.33it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 26it [00:01, 20.51it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 29it [00:01, 21.84it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 32it [00:01, 23.56it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 35it [00:01, 24.51it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 38it [00:01, 25.17it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 41it [00:01, 25.87it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 44it [00:02, 26.36it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 47it [00:02, 26.60it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 50it [00:02, 26.59it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 53it [00:02, 26.87it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 56it [00:02, 27.14it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 59it [00:02, 27.49it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 62it [00:02, 27.45it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 65it [00:02, 27.38it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 68it [00:02, 27.17it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 71it [00:03, 26.09it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 74it [00:03, 26.05it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 77it [00:03, 26.21it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 80it [00:03, 26.16it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 83it [00:03, 26.11it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 86it [00:03, 25.68it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 89it [00:03, 24.57it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 92it [00:03, 24.73it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 95it [00:03, 25.24it/s]\u001b[A\n",
      "Loss: 2.2472610473632812 Train: -1.3396346953249103 Test: -3.2978709281776 : 98it [00:04, 25.93it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 101it [00:04, 17.16it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 104it [00:04, 18.84it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 107it [00:04, 20.02it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 110it [00:04, 21.44it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 113it [00:04, 22.81it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 116it [00:04, 23.97it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 119it [00:05, 24.93it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 122it [00:05, 25.40it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 125it [00:05, 25.89it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 128it [00:05, 26.16it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 131it [00:05, 26.56it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 134it [00:05, 26.75it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 137it [00:05, 26.85it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 140it [00:05, 26.67it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 143it [00:05, 26.69it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 146it [00:06, 26.40it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 149it [00:06, 27.00it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 152it [00:06, 27.16it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 155it [00:06, 26.70it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 158it [00:06, 26.28it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 161it [00:06, 26.66it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 164it [00:06, 26.87it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 167it [00:06, 26.11it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 170it [00:06, 26.37it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 173it [00:07, 26.70it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 176it [00:07, 26.56it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 179it [00:07, 26.78it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 182it [00:07, 26.74it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 185it [00:07, 25.52it/s]\u001b[A\n",
      "Loss: 0.11750867962837219 Train: 0.8464955622592506 Test: 0.8586899906046013 : 188it [00:07, 23.68it/s]\u001b[A\n",
      "Loss: 0.05560298387092158 Train: 0.9336204999660637 Test: 0.9245614266426178 : 2620it [03:36, 12.09it/s][A\n",
      "Loss: 0.055597126483917236 Train: 0.9541863795146673 Test: 0.9235167656130201 : 3687it [02:29, 27.36it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-e1d7e15f74f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
    "\n",
    "pbar = tqdm(itertools.count(0))\n",
    "for step in pbar:\n",
    "    i = step % num_batches\n",
    "    # Populate feed_dict; duplicated code for num and cat\n",
    "    s = slice(batch_size * i, batch_size * (i + 1))\n",
    "    feed_dict = {nn_kp: 0.95,\n",
    "                 nn_lr: 0.001 if step < 800 else 0.0001,\n",
    "                 nn_X: X_train[s],\n",
    "                 nn_y: y_train[s, np.newaxis],\n",
    "                }\n",
    "    del i\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        _, l, y = sess.run([nn_train, nn_loss, nn_preds], feed_dict)\n",
    "        d = \"Loss: {} \".format(l)\n",
    "        d += \"Train: {} \".format(r_squared(y_train[s], y.ravel()))\n",
    "        y = sess.run(nn_preds, {nn_kp: 1.0,\n",
    "                                nn_X: X_test,\n",
    "                                nn_y: y_test[:, np.newaxis]})\n",
    "        d += \"Test: {} \".format(r_squared(y_test, y.ravel()))\n",
    "        pbar.set_description(d)\n",
    "    else:\n",
    "        sess.run(nn_train, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.summary.writer.writer.FileWriter at 0x7fe9f279e860>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "          \r",
      "Loss: 0.055597126483917236 Train: 0.9541863795146673 Test: 0.9235167656130201 : 3687it [02:48, 21.89it/s]"
     ]
    }
   ],
   "source": [
    "tf.summary.FileWriter(\"faceoff/\", graph=sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for KRR fitting: 0.549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, RBF\n",
    "import time\n",
    "\n",
    "# Fit KernelRidge with parameter selection based on 5-fold cross validation\n",
    "param_grid = {\"alpha\": [1e0, 1e-1, 1e-2, 1e-3],\n",
    "              \"kernel\": [RBF(l)\n",
    "                         for l in np.logspace(-2, 2, 10)]}\n",
    "kr = GridSearchCV(KernelRidge(), cv=5, param_grid=param_grid)\n",
    "stime = time.time()\n",
    "kr.fit(X_train[:100], y_train[:100])\n",
    "print(\"Time for KRR fitting: %.3f\" % (time.time() - stime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.42271147  0.10330646  0.66634543 ...,  0.54132327  0.54132327\n",
      "  0.54132327] [ 181586.72233875]\n",
      "[ 0.42271147  0.10330646  0.66634543 ...,  0.54132327  0.54132327\n",
      "  0.54132327] [ 180537.74127184]\n",
      "[ 0.42271147  0.10330646  0.66634543 ...,  0.54132327  0.54132327\n",
      "  0.54132327] [ 179500.65675795]\n",
      "[ 0.42271147  0.10330646  0.66634543 ...,  0.54132327  0.54132327\n",
      "  0.54132327] [ 178476.56694212]\n",
      "[ 0.42271147  0.10330646  0.66634543 ...,  0.54132327  0.54132327\n",
      "  0.54132327] [ 177466.08404523]\n",
      "[ 0.42271147  0.10330646  0.66634543 ...,  0.54132327  0.54132327\n",
      "  0.54132327] [ 176469.42592986]\n",
      "[ 0.42271147  0.10330646  0.66634543 ...,  0.54132327  0.54132327\n",
      "  0.54132327] [ 175486.5617425]\n",
      "[ 0.42271147  0.10330646  0.66634543 ...,  0.54132327  0.54132327\n",
      "  0.54132327] [ 174517.32678109]\n",
      "Caught KeyboardInterrupt, setting model                  with most recent state.\n"
     ]
    }
   ],
   "source": [
    "Z = np.random.rand(1000, X_train.shape[1])\n",
    "rbf = GPflow.kernels.RBF(X_train.shape[1], variance=1, ARD=True)\n",
    "m = GPflow.sgpr.SGPR(X_train, y_train[:,None], kern=rbf, Z=Z)\n",
    "def logger(x):\n",
    "    if (logger.i % 10) == 0:\n",
    "        print(x, m._objective(x)[0])\n",
    "    logger.i+=1\n",
    "logger.i = 0\n",
    "\n",
    "m.optimize(method=tf.train.AdamOptimizer(), callback=logger)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: -4.42959616572\n",
      "Test: -4.39652614283\n"
     ]
    }
   ],
   "source": [
    "y, _ = m.predict_y(X_train)\n",
    "print(\"Train:\", r_squared(y_train, y.flatten()))\n",
    "y, _ = m.predict_y(X_test)\n",
    "print(\"Test:\", r_squared(y_test, y.flatten()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
